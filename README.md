## Project Overview

The purpose of this analysis in terms of the activity is to help a friend determine performance of alternative energy stocks over the course of 2017 and 2018. In practice, we learned many foundational coding skills including but not limited to variables, conditionals, loops, and VBA Syntax. 

## Results

After analysis it looked like the year 2017 was a much more successfull year for most of the stocks in this dataset as shown by the screenshots below.

### 2017
<img width="311" alt="Screen Shot 2022-04-10 at 7 45 24 PM" src="https://user-images.githubusercontent.com/23485764/162660969-5635977d-a881-43fd-a3ea-240023409855.png">
### 2018
<img width="291" alt="Screen Shot 2022-04-10 at 7 45 44 PM" src="https://user-images.githubusercontent.com/23485764/162660971-95623c3e-cddb-48aa-b1bd-9ec37f4903d9.png">

During exercises that led to this outcome, I was able to dive deep into the data gain meaningful insights about key factors including starting price, ending price and net returns on the 12 companies highlighted by this dataset.  

## Summary

A significant challenge for me was working through syntax errors when it came to the refactoring of the code. Below are the times it took to run each year's analysis:

### 2017
![Screen Shot 2022-04-10 at 6 59 46 PM](https://user-images.githubusercontent.com/23485764/162661268-9121534a-9e8f-4a78-810e-87bf85f75e3d.png)

### 2018
![Screen Shot 2022-04-10 at 7 00 20 PM](https://user-images.githubusercontent.com/23485764/162661295-b97d5f37-42ef-4ed2-8f66-2b5bfb73eebe.png)

In terms of refactoring the code, the desired outcome is efficiency gains in order to reduce the strain on whatever system is running the analysis. In this case the gains seem minimal as the longest any of my runs took was less than half a second. However, I could understand if the dataset was far larger, the need for highly optimized code is larger than in an instance like this where the number of rows is relatively small. For example, this dataset for stocks only involves 11 companies and 6,026 total rows of data, split into 3,013 row chunks. Out in the "real world" the datasets have the potential to be far larger, ranging into the millions of rows of data. Therefore, the need for optimized code in a situation like that is quite high.
